before_script:
  - docker info
  - nvidia-smi


# template

.build:
  script:
    - docker build -t ${CI_JOB_ID} $CI_DOCKER_BUILD_ARGS .
    - docker run --rm --runtime nvidia --env JULIA_CUDA_MEMORY_LIMIT --env JULIA_DEBUG $CI_DOCKER_RUN_ARGS $CI_JOB_ID -e '
          using Pkg;
          run(`id`);
          run(`nvcc --version`);
          Pkg.test(collect(keys(Pkg.installed())));'
    - docker rmi ${CI_JOB_ID}


# CUDA versions

cuda:11.0:
  extends: .build
  variables:
    CI_DOCKER_BUILD_ARGS: '--build-arg IMAGE=nvidia/cuda:11.0-cudnn8-devel-ubuntu18.04'

cuda:10.2:
  extends: .build
  variables:
    CI_DOCKER_BUILD_ARGS: '--build-arg IMAGE=nvidia/cuda:10.2-cudnn7-runtime-ubuntu18.04'

cuda:10.1:
  extends: .build
  variables:
    CI_DOCKER_BUILD_ARGS: '--build-arg IMAGE=nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04'

cuda:10.0:
  extends: .build
  variables:
    CI_DOCKER_BUILD_ARGS: '--build-arg IMAGE=nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04'

cuda:9.2:
  extends: .build
  variables:
    CI_DOCKER_BUILD_ARGS: '--build-arg IMAGE=nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04'


# special tests

user:
  extends: .build
  variables:
    CI_DOCKER_RUN_ARGS: '--user 1000:1000'

debug:
  extends: .build
  variables:
    JULIA_DEBUG: 'CUDAapi,CUDAdrv,CUDAnative,CuArrays'
